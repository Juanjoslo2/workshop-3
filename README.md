# üåç An√°lisis con Streaming de datos y Machine Learning

Este proyecto, tiene como objetivo procesar y analizar datos de felicidad mundial de **2015 a 2019**. Utiliza t√©cnicas de **ETL (Extracci√≥n, Transformaci√≥n y Carga)**, *machine learning* para predecir puntajes de felicidad, y un flujo de datos en tiempo real con **Apache Kafka**, todo implementado mediante **Docker**.

---

## üìå Descripci√≥n General

El prop√≥sito del proyecto es doble:

- Comprender los factores que influyen en la felicidad global, como la econom√≠a, el apoyo social, la salud, la libertad, la percepci√≥n de corrupci√≥n y la generosidad.
- Entrenar un modelo de *machine learning* para predecir puntajes de felicidad utilizando datos hist√≥ricos limpios y transformados.

Se emplean herramientas modernas como **Python**, **Jupyter Notebook**, **PostgreSQL**, **Kafka** y **Scikit-learn**, integradas en un entorno **Dockerizado** para garantizar portabilidad y consistencia.

---

## ‚öôÔ∏è Requisitos

- Python 
- Jupyter Notebook
- PostgreSQL
- Apache Kafka
- Docker y Docker Compose
- Dependencias listadas en `requirements.txt`:
  - `pandas`, `scikit-learn`, `xgboost`, `kafka-python`, `SQLAlchemy`, entre otras.

---

## üöÄ Instalaci√≥n y Configuraci√≥n

```bash
# 1. Clonar el repositorio
git clone https://github.com/Juanjoslo2/workshop-3.git
cd workshop-3

# 2. Crear y activar un entorno virtual
python -m venv venv
# En Linux/Mac
source venv/bin/activate
# En Windows
venv\Scripts\activate

# 3. Instalar dependencias
pip install -r requirements.txt
```

### üìÅ Configurar variables de entorno

Crear un archivo `.env` dentro de `env/` con las credenciales de PostgreSQL:

```env
PG_USER=tu_usuario
PG_PASSWORD=tu_contrase√±a
PG_HOST=localhost
PG_PORT=5432
PG_DATABASE=nombre_base_datos
```

### üê≥ Levantar servicios con Docker

```bash
docker-compose up -d
```

---

## üóÇÔ∏è Estructura del Proyecto

```plaintext
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ best_model_xgboost_pipeline.pkl
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2015.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2016.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2017.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2018.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 2019.csv
‚îÇ   ‚îî‚îÄ‚îÄ transform_data/
‚îÇ       ‚îî‚îÄ‚îÄ transform_happiness_score.csv
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îî‚îÄ‚îÄ db.py
‚îú‚îÄ‚îÄ env/
‚îÇ   ‚îî‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_ETL.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 02_Model_training.ipynb
‚îú‚îÄ‚îÄ streaming/
‚îÇ   ‚îú‚îÄ‚îÄ producer.py
‚îÇ   ‚îî‚îÄ‚îÄ consumer.py
‚îî‚îÄ‚îÄ
```

---

## üßæ Descripci√≥n de Archivos

- `data/raw/`: Archivos CSV originales con datos de felicidad (2015-2019).
- `data/transform_data/transform_happiness_score.csv`: Datos consolidados y transformados.
- `data/model/best_model_xgboost_pipeline.pkl`: Modelo XGBoost entrenado y almacenado.
- `notebooks/01_ETL.ipynb`: Limpieza y transformaci√≥n de datos.
- `notebooks/02_Model_training.ipynb`: Entrenamiento y evaluaci√≥n de modelos.
- `database/db.py`: Operaciones de base de datos con PostgreSQL.
- `streaming/producer.py` / `consumer.py`: Scripts para transmisi√≥n de datos con Kafka.
- `docker-compose.yml`: Configuraci√≥n de Kafka y Zookeeper.

---

## üõ†Ô∏è Uso del Proyecto

### üß™ Extracci√≥n y Transformaci√≥n de Datos

- Ejecutar `notebooks/01_ETL.ipynb` para limpiar y transformar los archivos CSV en `data/raw/`.
- Se genera `transform_happiness_score.csv` en `data/transform_data/`.

### ü§ñ Entrenamiento del Modelo

- Ejecutar `notebooks/02_Model_training.ipynb` para entrenar los modelos y guardar el mejor (`XGBoost`) en `data/model/`.

### üîÅ Flujo de Datos con Kafka

1. Asegurarse de que Docker est√© corriendo:  
   ```bash
   docker-compose up -d
   ```

2. Crear un topic en Kafka (ej. `docker exec -it kafka_service kafka-topics --create --topic happiness_kafka_topic --bootstrap-server localhost:9092`).

3. Ejecutar el producer:  
   ```bash
   python streaming/producer.py
   ```

4. Ejecutar el consumer:  
   ```bash
   python streaming/consumer.py
   ```

---

## üìä Detalles T√©cnicos

### üßæ Datos Originales

| A√±o   | Columnas | Observaciones clave |
|-------|----------|---------------------|
| **2015** | 12 columnas | Incluye `Region` y m√©tricas detalladas como `Economy`, `Family`, `Trust`, etc. |
| **2016** | 13 columnas | Similar a 2015 pero con intervalos de confianza adicionales (`Lower/Upper Confidence Interval`). |
| **2017** | 12 columnas | Columnas con nombres diferentes (`Happiness.Score`, `Economy..GDP.per.Capita.`). No tiene `Region`. |
| **2018** | 9 columnas  | Estructura m√°s simplificada. Cambian nombres de columnas. Incluye un valor nulo en `Perceptions of corruption`. |
| **2019** | 9 columnas  | Igual estructura a 2018 pero sin valores nulos. No contiene `Region`. |

### üìÑ Detalle por archivo

#### **2015.csv**
- **N√∫mero de columnas**: 12  
- **Columnas**:
  - `Country` (object)
  - `Region` (object)
  - `Happiness Rank` (int64)
  - `Happiness Score` (float64)
  - `Standard Error` (float64)
  - `Economy (GDP per Capita)` (float64)
  - `Family` (float64)
  - `Health (Life Expectancy)` (float64)
  - `Freedom` (float64)
  - `Trust (Government Corruption)` (float64)
  - `Generosity` (float64)
  - `Dystopia Residual` (float64)

#### **2016.csv**
- **N√∫mero de columnas**: 13  
- **Columnas**:
  - Igual que 2015 +  
  - `Lower Confidence Interval` (float64)  
  - `Upper Confidence Interval` (float64)

#### **2017.csv**
- **N√∫mero de columnas**: 12  
- **Columnas**:
  - `Country` (object)
  - `Happiness.Rank` (int64)
  - `Happiness.Score` (float64)
  - `Whisker.high` (float64)
  - `Whisker.low` (float64)
  - `Economy..GDP.per.Capita.` (float64)
  - `Family` (float64)
  - `Health..Life.Expectancy.` (float64)
  - `Freedom` (float64)
  - `Generosity` (float64)
  - `Trust..Government.Corruption.` (float64)
  - `Dystopia.Residual` (float64)

#### **2018.csv**
- **N√∫mero de columnas**: 9  
- **Columnas**:
  - `Overall rank` (int64)
  - `Country or region` (object)
  - `Score` (float64)
  - `GDP per capita` (float64)
  - `Social support` (float64)
  - `Healthy life expectancy` (float64)
  - `Freedom to make life choices` (float64)
  - `Generosity` (float64)
  - `Perceptions of corruption` (float64) ‚Üí contiene un valor nulo

#### **2019.csv**
- **N√∫mero de columnas**: 9  
- **Columnas**:
  - Igual que 2018, pero sin valores nulos

### üßπ Preprocesamiento

- Eliminaci√≥n de columnas irrelevantes (`Happiness Rank`, `Dystopia Residual`).
- Renombramiento para uniformidad (ej. `GDP per capita` ‚Üí `economy`).
- Imputaci√≥n de valores nulos con la mediana.
- Asignaci√≥n de regiones faltantes desde a√±os anteriores.

> Resultado: `transform_happiness_score.csv`

---

## üìà Entrenamiento y Evaluaci√≥n

### Modelos utilizados:

- `LinearRegression`
- `DecisionTreeRegressor`
- `RandomForestRegressor`
- `GradientBoostingRegressor`
- `XGBoost`

### M√©tricas:

- `R¬≤`, `MAE`, `MSE`, `RMSE`, `Explained Variance`

**Mejor modelo:**  
‚úÖ **XGBoost** ‚Äî R¬≤ = **0.83**, MAE = **0.35**, RMSE = **0.45**

---

## üì° Integraci√≥n con Kafka

- **Producer**: Env√≠a filas de `transform_happiness_score.csv` al topic `happiness-transformed`.
- **Consumer**: Procesa mensajes, predice con XGBoost y guarda resultados en PostgreSQL.

---

## üóÉÔ∏è Base de Datos

Tabla: `predicted_happiness`

| Columna                   | Tipo de Dato           |
|--------------------------|------------------------|
| id                       | INTEGER PRIMARY KEY AUTOINCREMENT |
| region                   | TEXT                   |
| happiness_score          | FLOAT                  |
| economy                  | FLOAT                  |
| family                   | FLOAT                  |
| health                   | FLOAT                  |
| freedom                  | FLOAT                  |
| government_corruption    | FLOAT                  |
| generosity               | FLOAT                  |
| year                     | INTEGER                |
| predicted_happiness_score | FLOAT                 |

---

